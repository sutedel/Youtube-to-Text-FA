#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test Persian Text Normalizer with Real Texts
ØªØ³Øª Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø² Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
"""

import os
import glob
from persian_text_normalizer import PersianTextNormalizer


def test_with_real_texts():
    """Test the normalizer with real Persian texts from output directory"""
    
    print("=" * 70)
    print("ØªØ³Øª Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø² Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ")
    print("Testing Persian Text Normalizer with Real Texts")
    print("=" * 70)
    
    # Initialize normalizer
    normalizer = PersianTextNormalizer()
    
    # Find all .txt files in output directory
    txt_files = glob.glob("output/*.txt")
    
    if not txt_files:
        print("âŒ Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø¯Ø± Ù¾ÙˆØ´Ù‡ output Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")
        return
    
    print(f"ğŸ“ Ù¾ÛŒØ¯Ø§ Ø´Ø¯ {len(txt_files)} ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª")
    print()
    
    total_stats = {
        'files_processed': 0,
        'total_original_chars': 0,
        'total_normalized_chars': 0,
        'total_sentences': 0,
        'total_words': 0,
        'improvements': []
    }
    
    # Process each file
    for i, txt_file in enumerate(txt_files[:5], 1):  # Test first 5 files
        filename = os.path.basename(txt_file)
        print(f"ğŸ” ØªØ³Øª {i}: {filename}")
        print("-" * 50)
        
        try:
            # Read original text
            with open(txt_file, 'r', encoding='utf-8') as f:
                original_text = f.read()
            
            if not original_text.strip():
                print("âš ï¸  ÙØ§ÛŒÙ„ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª")
                continue
            
            # Analyze original text
            analysis = normalizer.analyze_text(original_text)
            
            # Show statistics
            print(f"ğŸ“Š Ø¢Ù…Ø§Ø± Ù…ØªÙ† Ø§ØµÙ„ÛŒ:")
            print(f"   Ø·ÙˆÙ„ Ù…ØªÙ†: {analysis['original_length']:,} Ú©Ø§Ø±Ø§Ú©ØªØ±")
            print(f"   Ø·ÙˆÙ„ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡: {analysis['normalized_length']:,} Ú©Ø§Ø±Ø§Ú©ØªØ±")
            print(f"   ØªØ¹Ø¯Ø§Ø¯ Ø¬Ù…Ù„Ø§Øª: {analysis['sentence_count']}")
            print(f"   ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª: {analysis['word_count']}")
            
            # Calculate improvement
            char_reduction = analysis['original_length'] - analysis['normalized_length']
            if char_reduction > 0:
                improvement = (char_reduction / analysis['original_length']) * 100
                print(f"   Ø¨Ù‡Ø¨ÙˆØ¯: {char_reduction:,} Ú©Ø§Ø±Ø§Ú©ØªØ± ({improvement:.1f}%)")
                total_stats['improvements'].append(improvement)
            
            # Show sample of normalized text
            normalized_text = analysis['normalized_text']
            sample_length = min(200, len(normalized_text))
            sample = normalized_text[:sample_length]
            
            print(f"\nğŸ“ Ù†Ù…ÙˆÙ†Ù‡ Ù…ØªÙ† Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡:")
            print(f"   {sample}...")
            
            # Show first few sentences
            if analysis['sentences']:
                print(f"\nğŸ“ Ù†Ù…ÙˆÙ†Ù‡ Ø¬Ù…Ù„Ø§Øª:")
                for j, sentence in enumerate(analysis['sentences'][:3], 1):
                    print(f"   {j}. {sentence}")
                if len(analysis['sentences']) > 3:
                    print(f"   ... Ùˆ {len(analysis['sentences']) - 3} Ø¬Ù…Ù„Ù‡ Ø¯ÛŒÚ¯Ø±")
            
            # Update total stats
            total_stats['files_processed'] += 1
            total_stats['total_original_chars'] += analysis['original_length']
            total_stats['total_normalized_chars'] += analysis['normalized_length']
            total_stats['total_sentences'] += analysis['sentence_count']
            total_stats['total_words'] += analysis['word_count']
            
            print()
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ {filename}: {e}")
            print()
    
    # Show overall statistics
    print("=" * 70)
    print("ğŸ“Š Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ")
    print("Overall Statistics")
    print("=" * 70)
    
    if total_stats['files_processed'] > 0:
        print(f"ğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡: {total_stats['files_processed']}")
        print(f"ğŸ“ Ú©Ù„ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ: {total_stats['total_original_chars']:,}")
        print(f"ğŸ“ Ú©Ù„ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡: {total_stats['total_normalized_chars']:,}")
        print(f"ğŸ“ Ú©Ù„ Ø¬Ù…Ù„Ø§Øª: {total_stats['total_sentences']:,}")
        print(f"ğŸ“ Ú©Ù„ Ú©Ù„Ù…Ø§Øª: {total_stats['total_words']:,}")
        
        # Calculate overall improvement
        total_char_reduction = total_stats['total_original_chars'] - total_stats['total_normalized_chars']
        if total_char_reduction > 0:
            overall_improvement = (total_char_reduction / total_stats['total_original_chars']) * 100
            print(f"ğŸ“ˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©Ù„ÛŒ: {total_char_reduction:,} Ú©Ø§Ø±Ø§Ú©ØªØ± ({overall_improvement:.1f}%)")
        
        if total_stats['improvements']:
            avg_improvement = sum(total_stats['improvements']) / len(total_stats['improvements'])
            print(f"ğŸ“Š Ø¨Ù‡Ø¨ÙˆØ¯ Ù…ØªÙˆØ³Ø·: {avg_improvement:.1f}%")
    
    print("\nâœ… ØªØ³Øª Ø¨Ø§ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯!")


def test_specific_file(filename):
    """Test a specific file in detail"""
    
    print(f"ğŸ” ØªØ³Øª Ø¬Ø²Ø¦ÛŒ ÙØ§ÛŒÙ„: {filename}")
    print("=" * 60)
    
    normalizer = PersianTextNormalizer()
    
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            original_text = f.read()
        
        # Show original text sample
        print("ğŸ“ Ù†Ù…ÙˆÙ†Ù‡ Ù…ØªÙ† Ø§ØµÙ„ÛŒ:")
        original_sample = original_text[:300]
        print(f"   {original_sample}...")
        print()
        
        # Normalize and analyze
        analysis = normalizer.analyze_text(original_text)
        
        # Show normalized text sample
        print("ğŸ“ Ù†Ù…ÙˆÙ†Ù‡ Ù…ØªÙ† Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡:")
        normalized_sample = analysis['normalized_text'][:300]
        print(f"   {normalized_sample}...")
        print()
        
        # Show detailed analysis
        print("ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø¬Ø²Ø¦ÛŒ:")
        print(f"   Ø·ÙˆÙ„ Ù…ØªÙ† Ø§ØµÙ„ÛŒ: {analysis['original_length']:,} Ú©Ø§Ø±Ø§Ú©ØªØ±")
        print(f"   Ø·ÙˆÙ„ Ù…ØªÙ† Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡: {analysis['normalized_length']:,} Ú©Ø§Ø±Ø§Ú©ØªØ±")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ Ø¬Ù…Ù„Ø§Øª: {analysis['sentence_count']}")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª: {analysis['word_count']}")
        
        if 'unique_words' in analysis:
            print(f"   Ú©Ù„Ù…Ø§Øª Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: {analysis['unique_words']}")
        
        # Show all sentences
        print(f"\nğŸ“ ØªÙ…Ø§Ù… Ø¬Ù…Ù„Ø§Øª ({len(analysis['sentences'])} Ø¬Ù…Ù„Ù‡):")
        for i, sentence in enumerate(analysis['sentences'], 1):
            print(f"   {i:2d}. {sentence}")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§: {e}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        # Test specific file
        filename = sys.argv[1]
        if os.path.exists(filename):
            test_specific_file(filename)
        else:
            print(f"âŒ ÙØ§ÛŒÙ„ {filename} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")
    else:
        # Test all files
        test_with_real_texts()
